{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13029221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7957dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.version)\n",
    "print(f'tf version : {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332ae77-72ae-42cc-8fb4-baa996ff3d96",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe5264",
   "metadata": {},
   "source": [
    "Data is from https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0551fd9-e4de-4566-be77-97ef69e8c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_data_path = '../datasets/h&m/' \n",
    "customer_df = pd.read_parquet(os.path.join(hm_data_path, 'customers.parquet'))\n",
    "int_df = pd.read_csv(os.path.join(hm_data_path, 'transactions_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73fcb7-ffd6-490c-b5f8-67b0c17dee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{int_df.memory_usage().sum():,} bytes\")\n",
    "print(int_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea81a8c-0200-4ffc-b20f-64641b8ac4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"memory(bytes) : {customer_df.memory_usage().sum():,}\")\n",
    "print(customer_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57850c4-7da2-42e3-973b-95a078928ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_cols = ['customer_id', 'club_member_status', 'fashion_news_frequency', 'age']\n",
    "customer_df = (customer_df[cust_cols]\n",
    "                .copy()\n",
    "                .dropna(subset=['age'])\n",
    "                .replace(['NONE', None], \"None\")\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a4524-fd38-4089-b2ad-0d3d4094ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = ['t_dat', 'customer_id', 'sales_channel_id','price']\n",
    "df = pd.merge(int_df[int_cols], customer_df, on=['customer_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef049a",
   "metadata": {},
   "source": [
    "methods for OHE : https://www.thetestspecimen.com/posts/one-hot-encoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sales_channel_id'] = df['sales_channel_id'].astype('string')\n",
    "df['t_dat'] = pd.to_datetime(df['t_dat'])\n",
    "df.sort_values(\"t_dat\", ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cda89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(len(df)*0.8)\n",
    "train_df = df.iloc[:train_index].copy()\n",
    "test_df = df.iloc[train_index:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90f112-c6ff-4d79-b788-b260413fe8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = ['club_member_status', 'fashion_news_frequency', 'sales_channel_id']\n",
    "num_cols = ['age']\n",
    "\n",
    "mm_scaler = MinMaxScaler()\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "fit_mm_scaler = mm_scaler.fit(train_df[num_cols])\n",
    "train_df[num_cols] = fit_mm_scaler.transform(train_df[num_cols])\n",
    "test_df[num_cols] = fit_mm_scaler.transform(test_df[num_cols])\n",
    "\n",
    "fit_oe = ord_enc.fit(train_df[ordinal_cols].values)\n",
    "train_df[ordinal_cols] = fit_oe.transform(train_df[ordinal_cols])\n",
    "test_df[ordinal_cols] = fit_oe.transform(test_df[ordinal_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8f3ed",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80961b46",
   "metadata": {},
   "source": [
    "## parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebe33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Save train/test into single parquet file\n",
    "single_parquet_path = '../datasets/h&m/pp_parquets/'\n",
    "\n",
    "# datetime to string just to replicate our data on the cloud.\n",
    "def save_single_parquet():\n",
    "    train_df.to_parquet(single_parquet_path + 'train_df.parquet', index=False)\n",
    "    test_df.to_parquet(single_parquet_path + 'test_df.parquet', index=False)\n",
    "save_single_parquet()\n",
    "\n",
    "# ==== Save train/test into multiple parquet with 128mb each\n",
    "# Train : single file is 595,894kb -> 580mb -> 116mb*5\n",
    "# Test : 151366 -> 147mb -> already in fairly optimal size.\n",
    "def save_multi_parquets(df, folder_path, chunk_size, dataset_type='train'):\n",
    "    for idx, df_chunk in enumerate(np.array_split(df, chunk_size)):\n",
    "        df_chunk.to_parquet(f'{folder_path}/{dataset_type}_df_{idx}.parquet', index=False)\n",
    "save_multi_parquets(train_df, os.path.join(single_parquet_path, 'optimal'),chunk_size=5)\n",
    "save_multi_parquets(train_df, os.path.join(single_parquet_path, 'multi'), chunk_size=200)\n",
    "\n",
    "save_multi_parquets(test_df, os.path.join(single_parquet_path, 'optimal'),\n",
    "                    chunk_size=2, dataset_type='test')\n",
    "save_multi_parquets(test_df, os.path.join(single_parquet_path, 'multi'), \n",
    "                    chunk_size=100, dataset_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096892d",
   "metadata": {},
   "source": [
    "## TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# whole df to single tfrecords\n",
    "# adding compression\n",
    "options = tf.io.TFRecordOptions(compression_type='GZIP', )\n",
    "single_tfrecord_path = '../datasets/h&m/tfrecords/'\n",
    "\n",
    "with tf.io.TFRecordWriter(os.path.join(single_tfrecord_path, 'test_data.tfrecord'),\n",
    "                          options=options) as writer:\n",
    "    for row in test_df.itertuples():\n",
    "        t_dat = getattr(row, 't_dat')\n",
    "        customer_id = getattr(row, 'customer_id')\n",
    "        sales_channel_id = getattr(row, 'sales_channel_id')\n",
    "        price = getattr(row, 'price')\n",
    "        club_member_status = getattr(row, 'club_member_status')        \n",
    "        fashion_news_frequency = getattr(row, 'fashion_news_frequency')                       \n",
    "        age = getattr(row, 'age')   \n",
    "        price = getattr(row, 'price')\n",
    "        feature_dict = {\n",
    "            't_dat':_bytes_feature(t_dat.encode('utf-8')),\n",
    "            'customer_id': _bytes_feature(customer_id.encode('utf-8')),\n",
    "            'sales_channel_id':_float_feature(sales_channel_id),\n",
    "            'club_member_status':_float_feature(club_member_status),\n",
    "            'fashion_news_frequency':_float_feature(fashion_news_frequency),\n",
    "            'age':_float_feature(age),\n",
    "            'price':_float_feature(price)\n",
    "        }\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "        writer.write(example_proto.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e36610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For saving multiple/optimal tfercord files\n",
    "\n",
    "single file tfrecord \n",
    "- train = 725mb -> 7 tfrecords\n",
    "- test = 182mb -> 2 tfrecords\n",
    "\"\"\"\n",
    "def write_tfrecord(df, file_path, folder_path):\n",
    "    with tf.io.TFRecordWriter(os.path.join(folder_path, file_path), options=options) as writer:\n",
    "        for row in df.itertuples():\n",
    "            t_dat = getattr(row, 't_dat')\n",
    "            customer_id = getattr(row, 'customer_id')\n",
    "            sales_channel_id = getattr(row, 'sales_channel_id')\n",
    "            price = getattr(row, 'price')\n",
    "            club_member_status = getattr(row, 'club_member_status')        \n",
    "            fashion_news_frequency = getattr(row, 'fashion_news_frequency')                       \n",
    "            age = getattr(row, 'age')   \n",
    "            price = getattr(row, 'price')\n",
    "            feature_dict = {\n",
    "                't_dat':_bytes_feature(t_dat.encode('utf-8')),\n",
    "                'customer_id': _bytes_feature(customer_id.encode('utf-8')),\n",
    "                'sales_channel_id':_float_feature(sales_channel_id),\n",
    "                'club_member_status':_float_feature(club_member_status),\n",
    "                'fashion_news_frequency':_float_feature(fashion_news_frequency),\n",
    "                'age':_float_feature(age),\n",
    "                'price':_float_feature(price)}\n",
    "            example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "            writer.write(example_proto.SerializePartialToString())\n",
    "\n",
    "optimal_tfrecord_folder_path = '../datasets/h&m/tfrecords/optimal'\n",
    "for idx, df_chunk in enumerate(np.array_split(train_df, 7)):\n",
    "    write_tfrecord(df_chunk, f'train_data_{idx}.tfrecord', optimal_tfrecord_folder_path)\n",
    "for idx, df_chunk in enumerate(np.array_split(test_df, 2)):\n",
    "    write_tfrecord(df_chunk, f'test_data_{idx}.tfrecord', optimal_tfrecord_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159dc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tfrecord_folder_path = '../datasets/h&m/tfrecords/multi'\n",
    "for idx, df_chunk in enumerate(np.array_split(train_df, 200)):\n",
    "    write_tfrecord(df_chunk, f'train_data_{idx}.tfrecord', multi_tfrecord_folder_path)\n",
    "    \n",
    "for idx, df_chunk in enumerate(np.array_split(test_df, 100)):\n",
    "    write_tfrecord(df_chunk, f'test_data_{idx}.tfrecord', multi_tfrecord_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180d163",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(4,))\n",
    "dense = tf.keras.layers.Dense(4, activation='relu')(inputs)\n",
    "dense = tf.keras.layers.Dense(4, activation='relu')(dense)\n",
    "dense = tf.keras.layers.Dense(4, activation='relu')(dense)\n",
    "outputs = tf.keras.layers.Dense(1, activation=None)(dense)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mean_squared_error,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics = [tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2e042",
   "metadata": {},
   "source": [
    "## petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configurations for petastorm\n",
    "\n",
    "\"\"\"\n",
    "from petastorm import make_reader, make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "features = ['age','sales_channel_id', 'club_member_status', 'fashion_news_frequency']\n",
    "batch_size = 512\n",
    "n_epochs = 3\n",
    "target_col = 'price'\n",
    "\n",
    "tr_reader_kwargs = {\n",
    "    'reader_pool_type':'thread',\n",
    "    'schema_fields' : features + ['price'],\n",
    "    'shuffle_rows':False,\n",
    "    'shuffle_row_groups':True,\n",
    "    'workers_count': 10\n",
    "}\n",
    "val_reader_kwargs = {\n",
    "    'reader_pool_type':'thread',\n",
    "    'schema_fields' : features + ['price'],\n",
    "    'shuffle_rows':False,\n",
    "    'shuffle_row_groups':False,\n",
    "    'workers_count': 10\n",
    "}\n",
    "\n",
    "curr_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_configs = {\n",
    "    \"use_multiprocessing\":False,\n",
    "    \"workers\":1,\n",
    "#     \"callbacks\":[tf.keras.callbacks.TensorBoard(log_dir=f'tb_logs/{curr_datetime}', profile_batch=(10, 15))]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b19ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(e):\n",
    "    tensors = []\n",
    "    for col_nm in features:\n",
    "        tensors.append(getattr(e, col_nm))\n",
    "    X = tf.cast(tf.stack(tensors, axis=1), tf.float32)\n",
    "    y = getattr(e, target_col)\n",
    "    return X, y\n",
    "\n",
    "def train_model_from_petastorm(tr_parquet_path, val_parquet_path, model):\n",
    "    start_time = time.perf_counter()\n",
    "    with make_batch_reader(tr_parquet_path, **tr_reader_kwargs) as tr_reader:      \n",
    "        with make_batch_reader(val_parquet_path, **val_reader_kwargs) as val_reader:\n",
    "            tr_dataset = make_petastorm_dataset(tr_reader).unbatch().batch(batch_size).map(parse_dataset)\n",
    "            val_dataset = make_petastorm_dataset(val_reader).unbatch().batch(batch_size).map(parse_dataset)\n",
    "            model.fit(tr_dataset, validation_data=val_dataset, epochs=n_epochs,\n",
    "                     **model_configs)\n",
    "    print(f\"Elapsed time: {time.perf_counter() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"file:///Users/haneu/Desktop/PROJECTS/datasets/h&m/pp_parquets\"\n",
    "folder_path = '../datasets/h&m/pp_parquets/'\n",
    "\n",
    "single_tr_parquet_path = os.path.join(base_url, \"train_df.parquet\")\n",
    "single_val_parquet_path = os.path.join(base_url, \"test_df.parquet\")\n",
    "\n",
    "optimal_tr_parquet_path = [f\"{base_url}/optimal/{f}\" for f in os.listdir(os.path.join(folder_path, \"optimal\"))\n",
    "                           if f.startswith(\"train\")]\n",
    "optimal_val_parquet_path = [f\"{base_url}/optimal/{f}\" for f in os.listdir(os.path.join(folder_path, \"optimal\")) \n",
    "                            if f.startswith(\"test\")]\n",
    "multi_tr_parquet_path = [f\"{base_url}/multi/{f}\" for f in os.listdir(os.path.join(folder_path, \"multi\")) if f.startswith(\"train\")]\n",
    "multi_val_parquet_path = [f\"{base_url}/multi/{f}\" for f in os.listdir(os.path.join(folder_path, \"multi\")) if f.startswith(\"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(single_tr_parquet_path))\n",
    "train_model_from_petastorm(single_tr_parquet_path, single_val_parquet_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(optimal_tr_parquet_path))\n",
    "train_model_from_petastorm(optimal_tr_parquet_path, optimal_val_parquet_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15419192",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(multi_tr_parquet_path))\n",
    "train_model_from_petastorm(multi_tr_parquet_path, multi_val_parquet_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d62d9",
   "metadata": {},
   "source": [
    "## TFrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only works for names dictionary,,,, 회사에선 둘다 된거 같은데;;;\n",
    "features = ['age','sales_channel_id', 'club_member_status', 'fashion_news_frequency']\n",
    "\n",
    "inputs = []\n",
    "for f in features:\n",
    "    inputs.append(tf.keras.layers.Input(shape=(1,), name=f))\n",
    "\n",
    "concat_input = tf.keras.layers.concatenate(inputs)\n",
    "dense = tf.keras.layers.Dense(4, activation='relu')(concat_input)\n",
    "dense = tf.keras.layers.Dense(4, activation='relu')(dense)\n",
    "dense = tf.keras.layers.Dense(4, activation='relu')(dense)\n",
    "outputs = tf.keras.layers.Dense(1, activation=None)(dense)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mean_squared_error,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics = [tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configurations for tfrecords\n",
    "\"\"\"\n",
    "def parse_tfrecord(serialized_example):\n",
    "    # example = tf.io.parse_single_example(serialized_example, feature_desc)\n",
    "    # for batch\n",
    "    example = tf.io.parse_example(serialized_example, feature_desc)\n",
    "    \n",
    "    # since it does not provide projection, need to store it w/o or exclude it manually\n",
    "    example.pop(\"t_dat\")\n",
    "    example.pop(\"customer_id\")\n",
    "    \n",
    "    y = example.pop('price')\n",
    "    return example, y\n",
    "\n",
    "feature_desc = {\n",
    "    \"t_dat\": tf.io.FixedLenFeature([], tf.string),\n",
    "    'customer_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'sales_channel_id': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'club_member_status': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'fashion_news_frequency': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'age': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'price': tf.io.FixedLenFeature([], tf.float32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f800435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_from_tfrecords(tr_data_paths, val_data_paths, model):\n",
    "    start_time = time.perf_counter()\n",
    "    tr_serialized_dataset = tf.data.TFRecordDataset(tr_data_paths, compression_type='GZIP')\n",
    "    tr_dataset = tr_serialized_dataset.shuffle(100_000).batch(batch_size).map(parse_tfrecord)\n",
    "    \n",
    "    val_serialized_dataset = tf.data.TFRecordDataset(val_data_paths, compression_type='GZIP')\n",
    "    val_dataset = val_serialized_dataset.batch(batch_size).map(parse_tfrecord)\n",
    "    model.fit(tr_dataset, validation_data=val_dataset, epochs=n_epochs,\n",
    "              **model_configs)\n",
    "    print(f\"Elapsed time: {time.perf_counter() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a017c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_single_tfrecord = '../datasets/h&m/tfrecords/train_data.tfrecord'\n",
    "val_single_tfrecord = '../datasets/h&m/tfrecords/test_data.tfrecord'\n",
    "\n",
    "tr_optimal_tfrecord = tf.io.gfile.glob('../datasets/h&m/tfrecords/optimal/train_*.tfrecord')\n",
    "val_optimal_tfrecord = tf.io.gfile.glob('../datasets/h&m/tfrecords/optimal/test_*.tfrecord')\n",
    "\n",
    "tr_multi_tfrecord = tf.io.gfile.glob('../datasets/h&m/tfrecords/multi/train_*.tfrecord')\n",
    "val_multi_tfrecord = tf.io.gfile.glob('../datasets/h&m/tfrecords/multi/test_*.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46afe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_from_tfrecords(tr_single_tfrecord, val_single_tfrecord, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8182ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_optimal_tfrecord))\n",
    "train_model_from_tfrecords(tr_optimal_tfrecord, val_optimal_tfrecord, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40765a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_multi_tfrecord))\n",
    "train_model_from_tfrecords(tr_multi_tfrecord, val_multi_tfrecord, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbfe52",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc89bb",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013e141",
   "metadata": {},
   "source": [
    "## spark-tensorflow-connector\n",
    "\n",
    "Writing tfrecords from pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = os.path.join(os.getcwd(), 'spark-tensorflow-connector-1.0.0-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88bda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .appName('stc-test')\\\n",
    "            .config('spark.jars', 'spark-tensorflow-connector-1.0.0-s_2.11.jar')\\\n",
    "            .getOrCreate()\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a73790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf = spark.createDataFrame(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c59112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://spark.apache.org/third-party-projects.html\n",
    "train_pdf.write.format('tfrecords').option('writeLocality', 'local').save(\"/tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4b84f",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e169dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4,10]))\n",
    "\n",
    "print(\"- - - - - - - - ds2 - - - - - - - -\")\n",
    "print(tf.random.uniform([4,10]))\n",
    "ds3 =(tf.random.uniform([4]),\n",
    "      tf.random.uniform([4, 4], maxval=100, dtype=tf.int32))\n",
    "\n",
    "print(\"- - - - - - - - ds3 - - - - - - - -\")\n",
    "print(ds3)\n",
    "ds3 = tf.data.Dataset.from_tensor_slices(ds3)\n",
    "\n",
    "# print(ds.element_spec)\n",
    "print(ds2.element_spec)\n",
    "print(ds3.element_spec)\n",
    "\n",
    "inc_ds = tf.data.Dataset.range(100)\n",
    "inc_ds2 = tf.data.Dataset.range(100, 200)\n",
    "comb_ds = tf.data.Dataset.zip((inc_ds, inc_ds2))\n",
    "batch_ds = comb_ds.batch(10)\n",
    "list(iter(comb_ds))[:3]\n",
    "\n",
    "for e in batch_ds: # this continues until all elements \n",
    "    print(e) \n",
    "    break\n",
    "    \n",
    "\n",
    "for e in batch_ds.take(2): # only take(n), n element within ds.\n",
    "    print(e)\n",
    "    \n",
    "\"\"\"\n",
    "Since each element may be different length, especially in sequential models. tf provide Dataset.padded_batch\n",
    "function\n",
    "\"\"\"\n",
    "# dataset = tf.data.Dataset.range(100)\n",
    "# dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "# dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
    "\n",
    "# for batch in dataset.take(2):\n",
    "#     print(batch.numpy())\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "198.496px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
