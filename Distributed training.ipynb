{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13029221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7957dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "tf version : 2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(f'tf version : {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc2711",
   "metadata": {},
   "source": [
    "# TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63eaf8",
   "metadata": {},
   "source": [
    "references\n",
    "- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd33359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"../datasets/h&m/hm_pp_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be0f4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47850, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ed959e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_year</th>\n",
       "      <th>prev_year_spend</th>\n",
       "      <th>prev_year_n_buy</th>\n",
       "      <th>age</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.422034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.422034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.371356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_year  prev_year_spend  prev_year_n_buy       age     price\n",
       "0               0.0              0.0              0.0  0.533333  0.422034\n",
       "1               0.0              0.0              0.0  0.500000  0.422034\n",
       "3               0.0              0.0              0.0  0.366667  0.371356"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "797850f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_year    float64\n",
       "prev_year_spend     float64\n",
       "prev_year_n_buy     float64\n",
       "age                 float64\n",
       "price               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a7533",
   "metadata": {},
   "source": [
    "## custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a01225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d7d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole df to single tfrecords\n",
    "\n",
    "# adding compression\n",
    "options = tf.io.TFRecordOptions(compression_type='GZIP', )\n",
    "\n",
    "with tf.io.TFRecordWriter('../datasets/h&m/train_all_not_serialized.tfrecord', options=None) as writer:\n",
    "    for row in train_df.itertuples():\n",
    "        year = getattr(row, 'transaction_year')\n",
    "        prev_year_spend = getattr(row, 'prev_year_spend')\n",
    "        prev_year_n_buy = getattr(row, 'prev_year_n_buy')\n",
    "        age = getattr(row, 'age')\n",
    "        price = getattr(row, 'price')\n",
    "        feature_dict = {\n",
    "            'year': _float_feature(year),\n",
    "            'prev_year_spend':_float_feature(prev_year_spend),\n",
    "            'prev_year_n_buy':_float_feature(prev_year_n_buy),\n",
    "            'age':_float_feature(age),\n",
    "            'price':_float_feature(price)\n",
    "        }\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "        writer.write(example_proto.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548e74d",
   "metadata": {},
   "source": [
    "parquet(388kB) -> TFRecord(6222kB)\n",
    "- is this increased size expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70634c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single row to single tfrecords\n",
    "feature_dict = {\n",
    "    'year': _float_feature(year),\n",
    "    'prev_year_spend':_float_feature(prev_year_spend),\n",
    "    'prev_year_n_buy':_float_feature(prev_year_n_buy),\n",
    "    'age':_float_feature(age),\n",
    "    'price':_float_feature(price)\n",
    "}\n",
    "example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "with tf.io.TFRecordWriter('../datasets/h&m/train_0_single.tfrecord') as writer:\n",
    "    writer.write(example_proto.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90dfc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading tfrecods\n",
    "\n",
    "feature_desc = {\n",
    "    \"year\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    'prev_year_spend': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'prev_year_n_buy': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'age': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'price': tf.io.FixedLenFeature([], tf.float32)}\n",
    "\n",
    "\n",
    "# single file\n",
    "serialized_dataset = tf.data.TFRecordDataset('../datasets/h&m/train_all.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1d96f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function parse at 0x00000206C0263168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse at 0x00000206C0263168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def parse(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_desc)\n",
    "    y = example.pop('price')\n",
    "    return example, y\n",
    "    \n",
    "dataset = serialized_dataset.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74b3a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tensorflow Linear Regression\n",
    "\n",
    "Does really bad on test set, values are way off compared to sklearn...\n",
    "\"\"\"\n",
    "inputs = tf.keras.layers.Input(shape=(4,))\n",
    "outputs = tf.keras.layers.Dense(1, activation=None)(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mean_squared_error,\n",
    "    optimizer = 'sgd',\n",
    "    metrics = [tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in dataset.take(1):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013e141",
   "metadata": {},
   "source": [
    "## spark-tensorflow-connector\n",
    "\n",
    "Writing tfrecords from pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "20d9fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = os.path.join(os.getcwd(), 'spark-tensorflow-connector-1.0.0-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b88bda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .appName('stc-test')\\\n",
    "            .config('spark.jars', 'spark-tensorflow-connector-1.0.0-s_2.11.jar')\\\n",
    "            .getOrCreate()\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9b91c9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "27a73790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf = spark.createDataFrame(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4349dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+---------------+------------------+------------------+\n",
      "|transaction_year|prev_year_spend|prev_year_n_buy|               age|             price|\n",
      "+----------------+---------------+---------------+------------------+------------------+\n",
      "|               0|            0.0|            0.0|0.5333333333333333|0.4220338983050847|\n",
      "|               1|            0.0|            0.0|               0.5|0.4220338983050847|\n",
      "+----------------+---------------+---------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "17c59112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://spark.apache.org/third-party-projects.html\n",
    "train_pdf.write.format('tfrecords').option('writeLocality', 'local').save(\"/tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80fb15",
   "metadata": {},
   "source": [
    "## petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5910e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e182c8be",
   "metadata": {},
   "source": [
    "# Petastorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3f49f",
   "metadata": {},
   "source": [
    "Loading straight from regular parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "17ebeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = '../datasets/h&m/transaction_train_sample.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "153584ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\haneu\\\\Desktop\\\\PROJECTS\\\\Finance'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('file://', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a6dd5b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\haneu\\\\Desktop\\\\PROJECTS\\\\Finance'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'file://' + os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fda8e1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\haneu\\\\Desktop\\\\PROJECTS\\\\Finance\\\\parquet_files'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), 'parquet_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a952bd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\haneu\\\\Desktop\\\\PROJECTS\\\\Finance\\\\parquet_files'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"file://\", os.path.join(os.getcwd(), 'parquet_files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8ff3e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../datasets/h&m/hm_pp_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "11cfad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_year</th>\n",
       "      <th>prev_year_spend</th>\n",
       "      <th>prev_year_n_buy</th>\n",
       "      <th>age</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.422034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.422034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.371356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072412</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.296525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_year  prev_year_spend  prev_year_n_buy       age     price\n",
       "0               0.0         0.000000             0.00  0.533333  0.422034\n",
       "1               0.0         0.000000             0.00  0.500000  0.422034\n",
       "3               0.0         0.000000             0.00  0.366667  0.371356\n",
       "6               0.0         0.000000             0.00  0.333333  0.337271\n",
       "8               0.0         0.072412             0.25  0.677778  0.296525"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "36ef7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.read.parquet(\"../datasets/h&m/hm_pp_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0a5576e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+---------------+------------------+------------------+-----------------+\n",
      "|transaction_year|prev_year_spend|prev_year_n_buy|               age|             price|__index_level_0__|\n",
      "+----------------+---------------+---------------+------------------+------------------+-----------------+\n",
      "|             0.0|            0.0|            0.0|0.5333333333333333|0.4220338983050847|                0|\n",
      "|             0.0|            0.0|            0.0|               0.5|0.4220338983050847|                1|\n",
      "|             0.0|            0.0|            0.0|0.3666666666666667|0.3713559322033898|                3|\n",
      "+----------------+---------------+---------------+------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3e8fdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.write.parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6e500782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/haneu/Desktop/PROJECTS/Finance/parquet_files\n"
     ]
    }
   ],
   "source": [
    "from petastorm import make_reader, make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "filepath = \"file:///Users/haneu/Desktop/PROJECTS/Finance/parquet_files\"\n",
    "print(filepath)\n",
    "# with make_reader(dataset_url=filepath) as reader:\n",
    "#     for row in reader:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "febe46bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function make_petastorm_dataset.<locals>.set_shape at 0x00000206C63F1A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function make_petastorm_dataset.<locals>.set_shape at 0x00000206C63F1A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:402: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  dataset = pq.ParquetDataset(path_or_paths, filesystem=fs, validate_schema=False, metadata_nthreads=10)\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:362: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if not dataset.common_metadata:\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\reader.py:422: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  filters=filters)\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\unischema.py:317: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  meta = parquet_dataset.pieces[0].get_metadata()\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\unischema.py:321: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  for partition in (parquet_dataset.partitions or []):\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:253: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  metadata = dataset.metadata\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:254: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  common_metadata = dataset.common_metadata\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:350: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  futures_list = [thread_pool.submit(_split_piece, piece, dataset.fs.open) for piece in dataset.pieces]\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:350: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  futures_list = [thread_pool.submit(_split_piece, piece, dataset.fs.open) for piece in dataset.pieces]\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\etl\\dataset_metadata.py:337: FutureWarning: ParquetDatasetPiece is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  for row_group in range(metadata.num_row_groups)]\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\tf_utils.py:134: UnicodeWarning: Tensorflow will convert all unicode strings back to bytes type. You may need to decode values.\n",
      "  \"You may need to decode values.\", UnicodeWarning)\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\arrow_reader_worker.py:140: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\arrow_reader_worker.py:288: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  partition_names = self._dataset.partitions.partition_names if self._dataset.partitions else set()\n",
      "C:\\Users\\haneu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\petastorm\\arrow_reader_worker.py:291: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  table = piece.read(columns=column_names - partition_names, partitions=self._dataset.partitions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred_schema_view(t_dat=<tf.Tensor: shape=(1, 100000), dtype=string, numpy=\n",
      "array([[b'2020-06-09', b'2019-10-12', b'2018-12-05', ..., b'2020-08-02',\n",
      "        b'2019-04-06', b'2020-04-01']], dtype=object)>, customer_id=<tf.Tensor: shape=(1, 100000), dtype=string, numpy=\n",
      "array([[b'b3c880da1436f617e6927b73ebbf4bcb1b14c2a3037bc7e939ecb71c8672e798',\n",
      "        b'd1e21026a54135afd647ec7508e99910bccc804bbd8a964994d090dae7d8590b',\n",
      "        b'e9d7b9fc3f2f2d5ef9d86dd6593085c9e1e6380a4c6af98233ff1762a29ed50b',\n",
      "        ...,\n",
      "        b'b637a3e7d8b0caa947aaefd609b8d84a9ee962cf0a52a51bac507ffc2bf1b741',\n",
      "        b'fe13ed72e20e33993616857eeb08137f12ad96144b017c5f4bc568eb92e9dc2c',\n",
      "        b'54a4b54f8b0ee58ac9f44e7e2c6b46efa3200524259366b5efad456631e38bb2']],\n",
      "      dtype=object)>, article_id=<tf.Tensor: shape=(1, 100000), dtype=int64, numpy=\n",
      "array([[695632099, 723353003, 657497011, ..., 768921001, 598517002,\n",
      "        761308002]], dtype=int64)>, price=<tf.Tensor: shape=(1, 100000), dtype=float64, numpy=\n",
      "array([[0.01583051, 0.01184746, 0.02540678, ..., 0.01354237, 0.01354237,\n",
      "        0.00845763]])>, sales_channel_id=<tf.Tensor: shape=(1, 100000), dtype=int64, numpy=array([[1, 2, 2, ..., 1, 2, 2]], dtype=int64)>)\n"
     ]
    }
   ],
   "source": [
    "with make_batch_reader(dataset_url_or_urls=filepath) as reader:\n",
    "    dataset = make_petastorm_dataset(reader)\n",
    "    dataset = dataset.shuffle(10_000).batch(512)\n",
    "    for e in dataset.take(1):\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f716725",
   "metadata": {},
   "source": [
    "Pyspark -> parquet -> tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40f711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2f4b84f",
   "metadata": {},
   "source": [
    "# Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e169dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4,10]))\n",
    "\n",
    "print(\"- - - - - - - - ds2 - - - - - - - -\")\n",
    "print(tf.random.uniform([4,10]))\n",
    "ds3 =(tf.random.uniform([4]),\n",
    "      tf.random.uniform([4, 4], maxval=100, dtype=tf.int32))\n",
    "\n",
    "print(\"- - - - - - - - ds3 - - - - - - - -\")\n",
    "print(ds3)\n",
    "ds3 = tf.data.Dataset.from_tensor_slices(ds3)\n",
    "\n",
    "# print(ds.element_spec)\n",
    "print(ds2.element_spec)\n",
    "print(ds3.element_spec)\n",
    "\n",
    "inc_ds = tf.data.Dataset.range(100)\n",
    "inc_ds2 = tf.data.Dataset.range(100, 200)\n",
    "comb_ds = tf.data.Dataset.zip((inc_ds, inc_ds2))\n",
    "batch_ds = comb_ds.batch(10)\n",
    "list(iter(comb_ds))[:3]\n",
    "\n",
    "for e in batch_ds: # this continues until all elements \n",
    "    print(e) \n",
    "    break\n",
    "    \n",
    "\n",
    "for e in batch_ds.take(2): # only take(n), n element within ds.\n",
    "    print(e)\n",
    "    \n",
    "\"\"\"\n",
    "Since each element may be different length, especially in sequential models. tf provide Dataset.padded_batch\n",
    "function\n",
    "\"\"\"\n",
    "# dataset = tf.data.Dataset.range(100)\n",
    "# dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "# dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
    "\n",
    "# for batch in dataset.take(2):\n",
    "#     print(batch.numpy())\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "198.496px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
